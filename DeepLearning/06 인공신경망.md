06 인공신경망
===

# 개요

ANN(Artifitial Neural Network)

# 2. 인공신경망

인공신경망에 대해 공부하기전에 생물학적 뉴런의 구조에 대해 알아보겠습니다. 아래 그림은 뉴런입니다. 뉴런은 수상 돌기들을 통해 자신에게 주어지는 자극을 받습니다. 각 자극들은 신경 세포체(cell body)에서 합해집니다. 간단한 처리를 거쳐 축색(Axon)을 통해 그 결과를 다른 뉴런으로 전달합니다. 축색의 끝엔은 시냅스라는 구조가 있어 뉴런의 활성화 상태를 연결된 다른 뉴런의 활동을 억제하거나 자극하는 전기적인 신호를 보냅니다.

## 뉴런의 동작

- 뉴런의 입력에는 억제성 입력이 있고, 흥분성 입력이 있습니다. 흥분성 입력은 억제 입력에 비해 충분히 큰 흥분성을 받아야 뉴런이 활성화됩니다. 학습은 하나의 뉴런이 다른 뉴런에 미치는 영향이 변경되도록 시냅스의 효과를 변형함으로써 발생합니다. 우리 뇌에는 약 10조개의 뉴런이 있습니다.

## 인공 뉴런

인공 뉴런은 생물학적 뉴런의 기능을 단순화해서 소프트웨어 적으로 구현한 것 입니다. n개의 입력을 받고, 각각의 입력에 가중치를 곱한 후 이 합을 구합니다. 이 합은 간단한 함수를 거쳐 값을 출력합니다. 그리고 이 결과 값은 다른 뉴런의 입력으로 전달됩니다. 

## 학습과 평가

신경망은 두가지 방식으로 동작합니다. 하나는 학습이며 다른 하나는 평가입니다. 학습에선은 입력으로 제시된 데이터의 특징, 즉 패턴을 학습합니다. 평가에서는 학습된 결과를 이용하여 새로운 데이터에 적용하여 예측합니다.

보통학습으로 사용되는 데이터의 양이 많고, 입력 데이터에 맞는 가중치를 찾기 위해 같은 데이터를 갖고 여러번 반복하기 때문에 학습 과정은 시간이 상당히 오래걸립니다. 평가에서는 도출된 가중치를 입력 데이터에 곱한후 간단한 함수를 적용하는 것에 불과하기 때문에 빠른 시간 내에 결과나 나옵니다.

# 3. 퍼셉트론(Perceptron)

퍼셉트론은 가장 단순한 ANN 아키텍처 중 하나로 Frank Rosenblatt가 1957년에 발명하였습니다. 워렌 매컬록과 월터 피트가 제안한 모델과는 약간 다른 인공 뉴런을 사용합니다. LTU(Linear Threshold Unit)이라고 하는 이 뉴런은 이진 값 대신에 숫자를 이용합니다. 앞에서 언급한 인공 뉴런의 구조는 동일하나 LTU는 입력의 가중합을 계산한 다음, 그 결과에 계단 함수를 적용합니다. 


퍼셉트론은 그림과 같이 입력 층과 여러 개의 LTU로 구성된 단일 층으로 구성됩니다. 출력은 아래와 같이 바이어스를 고려하여 입력들의 가중치 합이 일정 임계치 이상이면 1 그렇지 않으면 0으로 정해집니다.

$$1 if w_1 x_1 + w_2 x_2 + ... + w_n x_n + (-\theta)(1) >= 0$$
$$0 \;else$$

## 학습

학습은 주어진 입력에 대해 원하는 출력이 나오도록 하는 가중치를 자동으로 찾는 작업 또는 과정입니다. 가중치는 아래의 학습 규칙에 따라 조절합니다.

$$ w_{i,j} = w_{i,j} + n(y_j - y_j)x_i$$

- 실제 출력과 목표치와의 차이, 즉 오차에 입력 값을 곱한 값만큼 변경합니다.
- 오차가 음수이면 출력 값을 현재 보다 작게 하기 위해 가중치를 감소합니다.
- 반대로 오차가 양수이면 출력 값을 현재보다 크게 하기 위해 가중치를 증가합니다.

### 학습 속도 제어

학습율이 너무 낮으면 알고리즘이 최적의 솔루션에 도달하지만 시간이 오래걸립니다. 반대로 너무 높으면 알고리즘이 

## 예제

iris 데이터셋에 퍼셉트론을 적용한 예제를 한번 보겠습니다. iris 데이터셋은 원래 4개의 입력필드와 하나의 출력(0~2)로 구성되어 있습니다. 여기서는 3번째와 4번째의 입력필드만을 사용합니다. 출력도 원래 0인 출력은 1로, 나머지는 0으로 바꾸어 사용합니다.

Scikit-Learn 라이브러에서는 Perceptron 클래스를 제공하고 있습니다. Perceptron 객체를 생성한 후, fit 함수를 사용하여 학습합니다. 

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
x = iris.data[:,(2,3)]
y = (iris.target == 0).astype(np.int)

perceptron = Perceptron(random_state=30)
perceptron.fit(x,y)
predict = perceptron.predict([[2,0.5]])
print(predict)
```

## 한계

퍼셉트론의 경우 비선형 분리 문제에 적용할 수 가 없습니다. 두개의 그룹이 다양한 출력 값을 가질 경우 곡선 형태의 함수로 가중치를 표현해서 분리해야 합니다.

# 4. 다층신경망(Multi-Layer Perceptron)

다층신경망은 입력층과 출력층 사이에 하나 이상의 뉴런 층이 존재하는 신경망입니다. 수년 동안 많은 연구자들이 MLP를 성공적으로 훈련시키는 방벙블 찾았으며 1986년에 그 학습 알고리즘이 제안되었습니다. D. E. Rumelhar와 그 동료들이 제안한 역전파(backpropagation)학습 알고리즘입니다. 획기적인 학습 방법이며 오늘날의 딥러닝에서도 사용되고 있습니다.

## 역전파 학습

각 학습 인스턴스에 대해 먼저 예측(순방향)을 수행하고 오류를 측정합니다. 그리고 역순으로 각 뉴런 층을 통과하며 각 연결의 오류 기여도를 측정한 후 연결 가중치를 약간 조정하여 오류를 줄입니다. 역으로 전파된다 하여 역전파 학습입니다.

## 활성화 함수

MLP에서는 단계 함수 대신 Logistic 함수(ex. Sigmoid)를 사용합니다. Logistic 함수는 0 ~ 1 사이의 값을 , tanh 함수는 -1 ~ 1 사이의 값을 출력하는 함수입니다. Logistic 함수와 tanh 함수는 그 모양이 비슷하며, 전 구간에서 미분 가능합니다. ReLu는 이론적으로 z=0에서 미분을 할 수 없지만 실제로는 잘 작동합니다. ReLu는 계산 속도가 빠르다는 점과 최대 출력 값의 제한이 없습니다.

- 이진 분류

    - Logistic 함수

- 다중 분류

    - softmax 함수 : 각 뉴런의 출력 값 별로 함수에 적용하는 것이 아닌 모든 뉴런의 출력 값들을 고려하여 정규화하는 함수입니다.


        $\hat{P_k}(X)= \frac{exp(O_k(X))}{\displaystyle\sum_{j=1}^{N} exp(O_j(X))}$

----

분류 문제가 아닌 회귀 분석 문제의 경우 출력층에 활성화 함수를 적요하지 않으며, 즉 softmax도 적용하지 않습니다.

# 5. 경사 하강법(Gradient Descent)

일반적인 최적화 알고리즘이며 광범위한 문제에 대한 최적의 해결법을 제공하고 있습니다. 경사 하강법의 일반적인 개념은 비용 함수를 정의하고, 이 함수의 값을 최소화하기 위해 반복적으로 가중치를 조정합니다.

1. 가중치를 무작위 값으로 채웁니다.
2. 비용함수를 줄이려고 시도하면서 단계적으로 가중치의 값을 개선합니다.
3. 최소 비용이 나올때 까지 2번의 과정을 반복하게 됩니다.

## 예시

가중치는 x 하나만 있으며, 비용 함수를 $Cost(x) = x^2$로 정의합니다. 결국엔 $x^2$을 최소화하는 x를 찾는 문제가 됩니다. 우리는 바로 0이라는 것을 알지만 경사하강법을 이용한 과정을 보겟습니다.

1. 무작위로 x가 5로 설정하며 학습률은 0.2로 적용합니다.
2. 횟수를 반복하면서 비용 함수가 최소가 되는 지점을 찾습니다.
    
    $$x_1 = x_0 - 0.2 * 2 * x_0 = 5 - 0.2 * 2 * 5 = 5 - 2 = 3$$
    $$x_2 = x_1 - 0.2 * 2 * x_1 = 3 - 0.2 * 2 * 3 = 3 - 1.8 = 1.2$$

## 종류

경사하강법은 학습 데이터에 따라 다양한 경사 하강법을 적용할 수 있습니다.

- 일괄 경사 하강법(batch gradient descent)

    매 학습 단계마다 전체 학습 셋을 사용하여 경사를 계산하므로 학습 셋이 클 때는 느립니다.

- 스토케스틱 경사 하강법(SGD. Stochastic Gradient Descent)

    매 학습 단계마다 학습셋에서 무작위로 한 인스턴스를 선택하고, 그 인스턴스 만을 기반으로 경사를 계산합니다.

    알고리즘이 훨씬 빨라지며, 거대한 학습 셋을 훈련할 수 있는 장점이 있습니다. 확률론적 성질로 인해, 일괄 경사 하강법 보다 훨씬 덜 규칙적이고, 부드럽게 최적의 해를 찾지 못하고 최적의 해에 수렴하지 않게될 수 있는 단점이 있습니다.

- 미니 배치 경사 하강법(mini-batch gradient descent)

    이 두가지 방법의 중간 형태로 매 학습 단계 마다 미니 배치라고 불리는 작은 랜덤 집합의 인스턴스에 대해 경사를 계산합니다.

    SGD보다 GPU를 사용할 때 행렬 연산의 하드웨어 최적화로 인해 성능이 향상됩니다. SGD보다 조금 더 최적의 해에 갈 수 있다는 장점이 있는 반면, 그것으로 인해 지역 최적에서 벗어나기가 어려울 수 있습니다.

    > 지역 최적은 

# 6. 역전파 알고리즘

역전파 알고리즘은 순방향 패스와 역방향 패스로 구성됩니다. 순방향 패스에서는 현재의 연결 가중치를 기반으로 신경망의 실제 출력을 계산합니다. 역방향 패스에서는 목표치와 실제 출력을 이용하여 학습 오차를 계산하고, 이를 기반으로 각 뉴런 연결 가중치를 변경합니다. 연결 가중치를 변경할 때는 경사 하강법을 사용합니다.

## 학습 알고리즘

1. 각 뉴런의 연결 가중치를 무작위로 초기화합니다.
2. 학습 오류가 없거나 정해진 종료 조건을 만족할 때 까지 아래 과정을 반복합니다.

    - 학습 데이터의 한 인스턴스를 선택한 후 이를 입력 층에 제시한 후 순방향으로 진행하면서 각 뉴런들의 출력 값들을 계산합니다.
    - 출력층의 실제 출력과 목표치를 이용하여 학습 오차를 계산합니다.
    - 출력 층에서 입력 층 방향으로 진행하면서 계산된 오차를 바탕으로 각 뉴런 연결치의 변화량을 계산합니다.
    - 모든 뉴런의 연결 가중치 변화량 계산이 끝나면 실제로 각 연결 가중치의 값을 갱신합니다.
  
## 예제

